{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/usr/bin/python\n",
    "# # -*- coding: utf-8 -*-\n",
    "\n",
    "# import random\n",
    "# import numpy\n",
    "# import math\n",
    "# import string\n",
    "# import operator\n",
    "# from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "# # candidates to pay attention to\n",
    "# candidates = [\"TRUMP\", \"CRUZ\", \"RUBIO\", \"KASICH\"]\n",
    "\n",
    "# # n-gram lengths to iterate through\n",
    "# min_N = 1       # inclusive\n",
    "# max_N = 15      # exclusive\n",
    "\n",
    "\n",
    "\n",
    "# ####\n",
    "# ####  HELPER FUNCTIONS\n",
    "# ####\n",
    "\n",
    "# # returns a dict mapping each n-gram that appears in the corpus to its frequency in the corpus\n",
    "# def ngram_freqs(corpus, n):\n",
    "    \n",
    "#     # generate a list of all n-grams in the corpus\n",
    "#     ngrams = []\n",
    "#     for i in range(n, len(corpus)):\n",
    "#         if not \"<BR>\" in tuple(corpus[i-n:i]):\n",
    "#             ngrams += [tuple(corpus[i-n:i])]\n",
    "    \n",
    "#     # count the frequency of each n-gram\n",
    "#     freq_dict = defaultdict(int)\n",
    "#     for ngram in ngrams:\n",
    "#         freq_dict[ngram] += 1\n",
    "    \n",
    "#     return freq_dict\n",
    "\n",
    "# # combines two dicts by performing the provided operation on their values\n",
    "# def combine_dicts(a, b, op=operator.add):\n",
    "#     return dict(a.items() + b.items() + [(k, op(a[k], b[k])) for k in set(b) & set(a)])\n",
    "\n",
    "# # checks whether two n-grams overlap too much to include both\n",
    "# def overlap(a, b):\n",
    "#     max_overlap = min(3, len(a), len(b))\n",
    "#     overlap = False\n",
    "    \n",
    "#     # the begnning of a is in b\n",
    "#     if '-'.join(a[:max_overlap]) in '-'.join(b):\n",
    "#         overlap = True\n",
    "#     # the end of a is in b\n",
    "#     if '-'.join(a[-max_overlap:]) in '-'.join(b):\n",
    "#         overlap = True\n",
    "#     # the begnning of b is in a\n",
    "#     if '-'.join(b[:max_overlap]) in '-'.join(a):\n",
    "#         overlap = True\n",
    "#     # the end of b is in a\n",
    "#     if '-'.join(b[-max_overlap:]) in '-'.join(a):\n",
    "#         overlap = True\n",
    "    \n",
    "#     return overlap\n",
    "\n",
    "# ####\n",
    "# ####  ANALYSIS FUNCTIONS\n",
    "# ####\n",
    "\n",
    "# # returns a list of corpora, each a sequential list of all words said by one candidate\n",
    "# def corpus_list_from_file(filename):\n",
    "    \n",
    "#     # load all words from the file into memory\n",
    "#     words = open(filename).read().split()\n",
    "    \n",
    "#     # initialize the list of corpora\n",
    "#     corpus_list = []\n",
    "#     for candidate in candidates:\n",
    "#         corpus_list += [[]]\n",
    "    \n",
    "#     # iterate through words, putting them in the correct corpus\n",
    "#     speaker_index = -1\n",
    "    \n",
    "#     for word in words:\n",
    "        \n",
    "#         # change of speaker\n",
    "#         if word[-1] == \":\" and word.isupper():\n",
    "#             # name of the new speaker\n",
    "#             speaker = word[:-1]\n",
    "            \n",
    "#             # speaker is one of the candidates\n",
    "#             if speaker in candidates:\n",
    "#                 speaker_index = candidates.index(speaker)\n",
    "            \n",
    "#             # speaker is moderator or candidate not listed\n",
    "#             else:\n",
    "#                 speaker_index = -1\n",
    "            \n",
    "#             # add a speaking break indicator\n",
    "#             corpus_list[speaker_index] += [\"<BR>\"]\n",
    "        \n",
    "#         # regular word\n",
    "#         elif word[0] is not \"(\" and word[-1] is not \")\":\n",
    "            \n",
    "#             # remove punctuation and convert to lowercase\n",
    "#             word = word.translate(str.maketrans(\"\",\"\"), string.punctuation).lower()\n",
    "            \n",
    "#             if speaker_index >= 0:\n",
    "#                 if word is not \"\":\n",
    "#                     corpus_list[speaker_index] += [word]\n",
    "    \n",
    "#     return corpus_list\n",
    "\n",
    "# # returns a list of dicts, each mapping an n-gram to its frequency in the respective corpus\n",
    "# def freq_dicts_from_corpus_list(corpus_list):\n",
    "    \n",
    "#     # initialize the list of dicts\n",
    "#     freq_dicts = []\n",
    "#     for candidate in range(len(candidates)):\n",
    "#         freq_dicts += [defaultdict(int)]\n",
    "    \n",
    "#     # iteratively add all n-grams\n",
    "#     for n in range(min_N, max_N):\n",
    "#         for candidate in range(len(candidates)):\n",
    "#             corpus = corpus_list[candidate]\n",
    "#             dict_to_add = ngram_freqs(corpus, n)\n",
    "#             freq_dicts[candidate] = combine_dicts(freq_dicts[candidate], dict_to_add)\n",
    "    \n",
    "#     return freq_dicts\n",
    "\n",
    "# # returns a list of dicts, each mapping an n-gram to its tf-idf in the respective corpus\n",
    "# # see https://en.wikipedia.org/wiki/Tf-idf for further information\n",
    "# def tfidf_dicts_from_freq_dicts(freq_dicts):\n",
    "    \n",
    "#     # initialize the list of dicts\n",
    "#     tfidf_dicts = []\n",
    "#     for candidate in range(len(candidates)):\n",
    "#         tfidf_dicts += [defaultdict(int)]\n",
    "    \n",
    "#     # create a dict that maps an n-gram to the number of corpora containing that n-gram\n",
    "#     num_containing = defaultdict(int)\n",
    "#     for candidate in range(len(candidates)):\n",
    "#         for ngram in freq_dicts[candidate]:\n",
    "#             num_containing[ngram] += 1\n",
    "    \n",
    "#     # calculate tf-idf for each n-gram in each corpus\n",
    "#     for candidate in range(len(candidates)):\n",
    "#         for ngram in freq_dicts[candidate]:\n",
    "#             tf = freq_dicts[candidate][ngram]\n",
    "#             idf = math.log(len(candidates) / num_containing[ngram])\n",
    "            \n",
    "#             # normalize by length of n-gram\n",
    "#             tfidf_dicts[candidate][ngram] = tf * idf * len(ngram)\n",
    "            \n",
    "#             # kill anything ending in \"and\" \"or\" \"of\" \"with\"\n",
    "#             if ngram[-1] in [\"and\", \"or\", \"of\", \"with\"]:\n",
    "#                 tfidf_dicts[candidate][ngram] = 0\n",
    "\n",
    "#     return tfidf_dicts\n",
    "\n",
    "# # kills any phrase (tfidf=0) contained inside a larger phrase with a higher score\n",
    "# def prune_substrings(tfidf_dicts, prune_thru=1000):\n",
    "    \n",
    "#     pruned = tfidf_dicts\n",
    "    \n",
    "#     for candidate in range(len(candidates)):\n",
    "#         # growing list of n-grams in list form\n",
    "#         so_far = []\n",
    "        \n",
    "#         ngrams_sorted = sorted(tfidf_dicts[candidate].items(), key=operator.itemgetter(1), reverse=True)[:prune_thru]\n",
    "#         for ngram in ngrams_sorted:\n",
    "#             # contained in a previous aka 'better' phrase\n",
    "#             for better_ngram in so_far:\n",
    "#                 if overlap(list(better_ngram), list(ngram[0])):\n",
    "#                     #print \"PRUNING!! \"\n",
    "#                     #print list(better_ngram)\n",
    "#                     #print list(ngram[0])\n",
    "                    \n",
    "#                     pruned[candidate][ngram[0]] = 0\n",
    "#             # not contained, so add to so_far to prevent future subphrases\n",
    "#             else:\n",
    "#                 so_far += [list(ngram[0])]\n",
    "    \n",
    "#     return pruned \n",
    "\n",
    "# # sorts the n-grams for a candidate by tf-idf\n",
    "# def top_ngrams_for_candidate(tfidf_dicts, candidate, count=20):\n",
    "#     return sorted(tfidf_dicts[candidate].items(), key=operator.itemgetter(1), reverse=True)[:count]\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     corpus_list = corpus_list_from_file(\"gop_debate_all.txt\")\n",
    "#     freq_dicts = freq_dicts_from_corpus_list(corpus_list)\n",
    "#     tfidf_dicts = tfidf_dicts_from_freq_dicts(freq_dicts)\n",
    "#     tfidf_dicts = prune_substrings(tfidf_dicts)\n",
    "    \n",
    "#     # print the top ngrams sorted by tfidf\n",
    "#     for candidate in range(len(candidates)):\n",
    "#         print (candidates[candidate])\n",
    "#         for ngram in top_ngrams_for_candidate(tfidf_dicts, candidate, 400):\n",
    "#             print (ngram)\n",
    "    \n",
    "    \n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
