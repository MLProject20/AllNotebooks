{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from random import sample \n",
    "import re\n",
    "import glob as gb\n",
    "import pandas as pd \n",
    "import shutil as sh\n",
    "import os\n",
    "from selenium import webdriver\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.selector import Selector\n",
    "import from crochet import setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\padu\\\\Desktop\\\\Zillow\\\\\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zillow scraper class\n",
    "class ZillowScraper(scrapy.Spider):\n",
    "    # scraper/spider name\n",
    "    name = 'zillow'\n",
    "    \n",
    "    # base URL\n",
    "    base_url = 'https://www.zillow.com/charlotte-nc/rentals/2_p/?'\n",
    "    \n",
    "    # custom headers\n",
    "    headers = {\n",
    "         'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.190 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # string query parameters\n",
    "    params = {\n",
    "       'searchQueryStae': '{\"pagination\":{\"currentPage\":2},\"usersSearchTerm\":\"Charlotte,NC\",\"mapBounds\":{\"west\":-81.05677437207031,\"east\":-80.60496162792968,\"south\":34.99121373318643,\"north\":35.427128457340764},\"regionSelection\":[{\"regionId\":24043,\"regionType\":6}],\"isMapVisible\":true,\"filterState\":{},\"isForSaleByOwner\":{\"value\":false},\"isForRent\":{\"value\":true},\"isAllHomes\":{\"value\":true},\"isListVisible\":true}'\n",
    "    }\n",
    "    \n",
    "    # custom settings\n",
    "    custom_settings = {\n",
    "        # uncomment below settings to slow down the scraper\n",
    "        'CONCURRENT_REQUESTS_PER_DOMAIN': 2,\n",
    "        'DOWNLOAD_DELAY': 1\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # crawler's entry point\n",
    "def start_requests(self):\n",
    "    # loop over page range \n",
    "    for page in range(0, 21,1):\n",
    "        # parse params\n",
    "        parsed_params = json.loads(self.params['searchQueryState'])\n",
    "\n",
    "        # init next page\n",
    "        parsed_params['pagination']['currentPage'] = str(page)\n",
    "\n",
    "        # update string query parameters\n",
    "        self.params['searchQueryState'] = json.dumps(parsed_params).replace(' ', '')\n",
    "\n",
    "        # init next page URL\n",
    "        next_page = self.base_url + urllib.parse.urlencode(self.params)\n",
    "\n",
    "        # crawl next page URL\n",
    "        yield scrapy.Request(url=next_page, headers=self.headers, callback=self.parse_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " # parse card links\n",
    "def parse_links(self, res):\n",
    "    # extract card links\n",
    "    card_links = res.css('ul[class=\"photo-cards photo-cards_wow photo-cards_short\"]')\n",
    "    card_links = card_links.css('li')\n",
    "    card_links = card_links.css('a.list-card-link::attr(href)')\n",
    "\n",
    "    # loop over property cards\n",
    "    for card in card_links:\n",
    "        # crawl property listing page recursively\n",
    "        yield res.follow(url=card.get(), headers=self.headers, callback=self.parse_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "           \n",
    "# parse property listing\n",
    "def parse_listing(self, res):\n",
    "    \n",
    "    # store listing HTML to local file\n",
    "    with open('res.html', 'w') as f:\n",
    "        f.write(res.text)\n",
    "\n",
    "\n",
    "    \n",
    "    # local listing HTML content\n",
    "    content = ''\n",
    "\n",
    "    # load local listing HTML file to extract data from it\n",
    "    with open('res.html', 'r') as f:\n",
    "        for line in f.read():\n",
    "            content += line\n",
    "\n",
    "    # init scrapy selector\n",
    "    res = Selector(text=content)\n",
    "    \n",
    "\n",
    "    # extract feature list\n",
    "    features = {\n",
    "        'price': ''.join(res.css('div[class=\"ds-chip\"]')\n",
    "                              .css('h3[class=\"ds-price\"] *::text')\n",
    "                              .getall()),\n",
    "\n",
    "        'address': ''.join(res.css('div[class=\"ds-chip\"]')\n",
    "                              .css('h1[class=\"ds-address-container\"] *::text')\n",
    "                              .getall()),\n",
    "\n",
    "        'bedrooms': ' '.join(res.css('div[class=\"ds-chip\"]')\n",
    "                              .css('span[class=\"ds-bed-bath-living-area\"] *::text')\n",
    "                              .getall())\n",
    "                              .replace('  ', '|')\n",
    "                              .replace('| ', ':')\n",
    "                              .split()[0]\n",
    "                              .replace(':bd', ''),\n",
    "\n",
    "        'bathrooms': ' '.join(res.css('div[class=\"ds-chip\"]')\n",
    "                  .css('span[class=\"ds-bed-bath-living-area\"] *::text')\n",
    "                  .getall())\n",
    "                  .replace('  ', '|')\n",
    "                  .replace('| ', ':')\n",
    "                  .split()[1]\n",
    "                  .replace(':ba', ''),\n",
    "\n",
    "        'floor_area': ' '.join(res.css('div[class=\"ds-chip\"]')\n",
    "                  .css('span[class=\"ds-bed-bath-living-area\"] *::text')\n",
    "                  .getall())\n",
    "                  .replace('  ', '|')\n",
    "                  .replace('| ', ':')\n",
    "                  .split()[2]\n",
    "                  .replace(':', ' '),\n",
    "\n",
    "        'zestimate': ''.join(res.css('div[class=\"ds-chip\"]')\n",
    "                                .css('div[class=\"ds-chip-removable-content\"] *::text')\n",
    "                                .getall())\n",
    "                                .split('\\u00ae:\\u00a0')[-1],\n",
    "\n",
    "        'description': res.css('div[class=\"Text-aiai24-0 sc-feJyhm erkQcD\"]::text')\n",
    "                          .get(),\n",
    "\n",
    "        'agent_info': {\n",
    "\n",
    "            'agent_name': res.css('ul[class=\"cf-listing-agent-info\"] *::text')\n",
    "                             .getall()[0],                              \n",
    "\n",
    "            'agent_phone': res.css('ul[class=\"cf-listing-agent-info\"] *::text')\n",
    "                             .getall()[1]\n",
    "        },\n",
    "\n",
    "        'facts_and_features': {},\n",
    "\n",
    "        'tax_history': {},\n",
    "\n",
    "        'monthly_cost': {},\n",
    "\n",
    "        'nearby_schools': [\n",
    "                            ' '.join(Selector(text=school).css(' *::text').getall()) for school in\n",
    "                            res.css('ul[class=\"ds-nearby-schools-list\"]')\n",
    "                               .css('li[class=\"sc-cMhqgX ikQQNx\"]')\n",
    "                               .getall()\n",
    "                          ],\n",
    "\n",
    "        'coordinates': {\n",
    "            'latitude': '',\n",
    "            'longitude': ''\n",
    "        }\n",
    "\n",
    "    }\n",
    "\n",
    "    # try to extract facts and features\n",
    "    try:\n",
    "        facts = res.css('ul[class=\"ds-home-fact-list\"]')\n",
    "        facts = '|'.join(facts.css('li *::text').getall()).replace(':|', ':').split('|')\n",
    "\n",
    "        # loop over facts\n",
    "        for fact in facts:\n",
    "            features['facts_and_features'][fact.split(':')[0]] = fact.split(':')[1]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # try to extract tax history\n",
    "    try:\n",
    "        tax_history = ''.join([' '.join(Selector(text=ul).css('li *::text').getall()) for ul in res.css('ul[class=\"sc-dqBHgY kQzYMy\"]').getall() if 'Tax assessed value:' in ul])\n",
    "\n",
    "        # store tax history\n",
    "        features['tax_history'] = {\n",
    "            'Tax assessed value': tax_history.split('Tax assessed value:')[1].split()[0],\n",
    "            'Annual tax amount': tax_history.split('Annual tax amount:')[1].split()[0]\n",
    "        }\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # try to extract monthly cost\n",
    "    try:\n",
    "        monthly_cost = res.css('div[class=\"sc-1b8bq6y-6 kKSvPL\"] *::text').getall()\n",
    "        monthly_cost = '|'.join(monthly_cost).replace('|$', ':$').replace('Chevron Down', '')\n",
    "        monthly_cost = [item.replace('|', ' ').strip().replace('Utilities ', 'Utilities:') for item in monthly_cost.split('||')]\n",
    "\n",
    "        # loop over monthly cost table\n",
    "        for item in monthly_cost:\n",
    "            features['monthly_cost'][item.split(':')[0]] = item.split(':')[1]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # try to extract coordinates\n",
    "    script = [script for script in res.css('script[type=\"application/ld+json\"]').getall() if 'latitude' in script][0]\n",
    "    script = Selector(text=script).css('::text').get()\n",
    "    script = json.loads(script)\n",
    "\n",
    "    # store coordinates\n",
    "    features['coordinates']['latitude'] = script['geo']['latitude']\n",
    "    features['coordinates']['longitude'] = script['geo']['longitude']\n",
    "\n",
    "    # store output to JSON file\n",
    "    with open('zillow.jsonl', 'a') as f:\n",
    "        f.write(json.dumps(features, indent=2) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-05 12:54:43 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: scrapybot)\n",
      "2021-03-05 12:54:43 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.2.0, Python 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Windows-10-10.0.18362-SP0\n",
      "2021-03-05 12:54:43 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2021-03-05 12:54:43 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'CONCURRENT_REQUESTS_PER_DOMAIN': 2, 'DOWNLOAD_DELAY': 1}\n",
      "2021-03-05 12:54:43 [scrapy.extensions.telnet] INFO: Telnet Password: e664728075a903b5\n",
      "2021-03-05 12:54:43 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-03-05 12:54:43 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-03-05 12:54:43 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-03-05 12:54:43 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-03-05 12:54:43 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-03-05 12:54:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-03-05 12:54:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-bbb13cdf3407>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprocess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrawlerProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZillowScraper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#     debug data extraction logic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scrapy\\crawler.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self, stop_after_crawl)\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[0mtp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjustPoolsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxthreads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'REACTOR_THREADPOOL_MAXSIZE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[0mreactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddSystemEventTrigger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'before'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shutdown'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m         \u001b[0mreactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocking call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_graceful_stop_reactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1421\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1422\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1423\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1402\u001b[0m         \"\"\"\n\u001b[0;32m   1403\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_installSignalHandlers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1404\u001b[1;33m         \u001b[0mReactorBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReactorBase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_reallyStartRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    841\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 843\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stopped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main driver\n",
    "if __name__ == '__main__':\n",
    "    # run scraper\n",
    "    process = CrawlerProcess()\n",
    "    process.crawl(ZillowScraper)\n",
    "    process.start()\n",
    "    \n",
    "#     debug data extraction logic\n",
    "    ZillowScraper.parse_listing(ZillowScraper, '')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\padu\\\\Desktop\\\\Zillow'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
